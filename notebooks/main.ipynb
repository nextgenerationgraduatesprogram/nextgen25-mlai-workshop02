{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3781f022",
   "metadata": {},
   "source": [
    "<h1>ü§ñ MLAI Workshop #01</h1>\n",
    "\n",
    "Summary of the workshop and key lessons.\n",
    "\n",
    "üí¨ *Question for the audience! ...?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112cacd",
   "metadata": {},
   "source": [
    "<h2>üóìÔ∏è Agenda</h2>\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26165351",
   "metadata": {},
   "source": [
    "<h2>Summary</h2>\n",
    "\n",
    "1. Functions describe the world.\n",
    "2. Hypothesis space describes the function we can learn.\n",
    "3. Dataset limits how we we can approximate the function.\n",
    "4. Optimization decides what function we end up with\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/raw/main/media/notebook/NetworkSpaces.jpg\" height=\"400\"/>\n",
    "    <p><em>Figure 1. Describe spaces.</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee8175",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c49551",
   "metadata": {},
   "source": [
    "<h2>üìà Section 1: What's wrong with our hypothesis space?</h2>\n",
    "\n",
    "There is some target function we want to approximate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77115786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target function\n",
    "def f_target(x):\n",
    "    return -0.68 * x + 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8caf9",
   "metadata": {},
   "source": [
    "We might observe and collect some data on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe some data using a noisy observation process\n",
    "def observe_noisy(f, x):\n",
    "    # we often can't sample perfectly a point in the domain perfectly\n",
    "    x_noise = np.random.normal(0.05, 0.01, x.shape[0])\n",
    "    x_measure = x + x_noise\n",
    "\n",
    "    # we often can't measure the result perfectly\n",
    "    y_noise = np.random.normal(-0.05, 0.09, x.shape[0])\n",
    "    y_obs = f(x_measure) + y_noise\n",
    "\n",
    "    return y_obs\n",
    "\n",
    "# we select some points to observe the response at\n",
    "x_meas = np.random.uniform(low=-1, high=1, size=100)\n",
    "y_obs = observe_noisy(f_target, x_meas)\n",
    "\n",
    "# plot the observations\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
    "ax.set_xlim(x_meas.min(), x_meas.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x_meas\")\n",
    "ax.set_ylabel(\"y_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137855a7",
   "metadata": {},
   "source": [
    "We might hypothesize this has a linear relationship and construct a hypothesis space to match this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a hypothesis space\n",
    "def H(a, b):\n",
    "    def f(x):\n",
    "        return a * x + b\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d593e",
   "metadata": {},
   "source": [
    "We construct a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "def dataset_avg_mse_loss(f, x, y):\n",
    "    # compute average mse loss between prediction `f(x)` and observation `y`\n",
    "    L = np.sum((f(x) - y) ** 2) / x.shape[0]\n",
    "    return L\n",
    "\n",
    "# try a specific function\n",
    "L_mse = dataset_avg_mse_loss(H(1,1), x_meas, y_obs)\n",
    "L_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096900db",
   "metadata": {},
   "source": [
    "We define the set of suitable candidates based on this loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if the solution is suitable\n",
    "def is_suitable(L_mse, threshold):\n",
    "    return L_mse < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f391de1",
   "metadata": {},
   "source": [
    "We then start from an initial set of parameters and optimize them through our loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss landscape for understanding\n",
    "a = np.linspace(-5, 5, num=100) # <-- explore a different range, does [-5,5] let you find the optimal solution?\n",
    "b = np.linspace(-5, 5, num=100)\n",
    "A, B = np.meshgrid(a, b, indexing=\"ij\")\n",
    "\n",
    "# lets compute the loss (error across the dataset) for each function defined by the parameter\n",
    "losses = np.zeros_like(A)\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        # select a function from the hypothesis space\n",
    "        f_approx = H(a=a[i], b=b[j])\n",
    "\n",
    "        # compute loss across dataset\n",
    "        losses[i,j] = dataset_avg_mse_loss(f_approx, x_meas, y_obs)\n",
    "\n",
    "# lets plot the result\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "s = ax.plot_surface(A, B, losses, cmap='viridis')\n",
    "ax.set_xlim(a.min(), a.max())\n",
    "ax.set_ylim(b.min(), b.max())\n",
    "ax.set_xlabel(\"a\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"Dataset Loss (MSE)\")\n",
    "ax.set_title(f\"Loss Landscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivate of function with respect to parameters\n",
    "def df_da(x, a, b):\n",
    "    return x\n",
    "\n",
    "def df_db(x, a, b):\n",
    "    return 1\n",
    "\n",
    "# derivate of loss function with respect to params : chain rule\n",
    "def dL_dtheta(x, y, a, b):\n",
    "    dL_da = 2 * np.mean((H(a, b)(x) - y) * df_da(x, a, b))\n",
    "    dL_db = 2 * np.mean((H(a, b)(x) - y) * df_db(x, a, b))\n",
    "    return (dL_da, dL_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb47a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization using gradient descent\n",
    "def gradient_descent(a_init, b_init, x, y, lr, steps, thresh):\n",
    "    a, b = a_init, b_init\n",
    "    history = []\n",
    "\n",
    "    for idx in range(steps):\n",
    "        # compute gradients\n",
    "        dL_da, dL_db = dL_dtheta(x, y, a, b)\n",
    "\n",
    "        # curr loss\n",
    "        L_mse = dataset_avg_mse_loss(H(a, b), x, y)\n",
    "        history.append((a, b, L_mse))\n",
    "\n",
    "        # exit condition : found suitable solution\n",
    "        if is_suitable(L_mse, thresh):\n",
    "            print(f\"found suitable solution (a={a:.2f}, b={b:.2f}) with loss L={L_mse:.3f} after {idx} steps\") \n",
    "            break\n",
    "\n",
    "        # gradient descent update step\n",
    "        a = a - lr * dL_da\n",
    "        b = b - lr * dL_db\n",
    "\n",
    "    return np.array(history)\n",
    "\n",
    "# run gradient descent and get final parameters\n",
    "trajectory = gradient_descent(a_init=.0, b_init=.0, x=x_meas, y=y_obs, lr=0.1, steps=100, thresh=0.001)\n",
    "f_approx = H(trajectory[-1,0], trajectory[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over iterations\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(trajectory[:, 2], label=\"L_mse\")\n",
    "ax.set_xlim(left=0, right=len(trajectory)-1)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss (MSE)\")\n",
    "ax.set_title(\"Loss Curve\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Plot the path on the loss landscape\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(A, B, losses, levels=100, cmap='viridis')\n",
    "ax.set_xlim(A.min(), A.max())\n",
    "ax.set_ylim(B.min(), B.max())\n",
    "ax.set_xlabel(\"a\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"Gradient Descent Path on Loss Landscape\")\n",
    "\n",
    "# Path\n",
    "ax.plot(trajectory[0, 0], trajectory[0, 1], 'rx', label=\"Initial Point\")\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', alpha=0.25, label=\"Gradient Descent Path\")\n",
    "ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', label=\"Final Point\")\n",
    "ax.legend()\n",
    "\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.scatter(x_meas, f_approx(x_meas), s=6, label=\"predictions\")\n",
    "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81505ef",
   "metadata": {},
   "source": [
    "We have to manually define the function for the gradients and define the optimization algorithm, instead we can use the `torch` library with its autograd engine and pre-defined optimizers to make our life much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HypothesisSpace(nn.Module):\n",
    "    \"\"\" implement a simple linear equation hypothesis space\n",
    "    f(x) = a * x + b\n",
    "    \"\"\"\n",
    "    def __init__(self, a: float, b: float):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.tensor(a))\n",
    "        self.b = nn.Parameter(torch.tensor(b))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.a * x + self.b\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.a.item():.2f} x^2 + {self.b.item():.2f} x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b021fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization loop aka. training loop\n",
    "def training_loop(model, optimizer, loss_fn, x, y, steps, thresh = None):\n",
    "    # training loop\n",
    "    trajectory = []\n",
    "    for idx in range(steps):\n",
    "        # clear gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # performs a forwards pass : constructs the computation graph for gradients\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute the loss function : adds more nodes to the graph\n",
    "        loss = loss_fn(y, y_pred)\n",
    "\n",
    "        # store for visualization\n",
    "        trajectory.append([p.item() for p in model.parameters()] + [loss])\n",
    "\n",
    "        # break\n",
    "        if thresh is not None:\n",
    "            if is_suitable(loss, thresh):\n",
    "                print(f\"found suitable model with loss L={loss.item():.3f} after {idx} iterations\")\n",
    "                break\n",
    "\n",
    "        # performs a backwards pass : computes the gradients across the graph i.e. dL/df -> df/da -> ...\n",
    "        loss.backward()\n",
    "\n",
    "        # step optimizer : uses the gradients to perform a gradient descent step i.e. a = a - lr * dL/da\n",
    "        optimizer.step()\n",
    "\n",
    "    trajectory = torch.tensor(trajectory)\n",
    "    return model, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to tensors\n",
    "x_meas_tensor = torch.from_numpy(x_meas)\n",
    "y_obs_tensor = torch.from_numpy(y_obs)\n",
    "\n",
    "# initial model\n",
    "model = HypothesisSpace(1.0, 1.0)\n",
    "\n",
    "# define optimizer : autograd handles computing gradients etc.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# define loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# perform the training loop\n",
    "model, trajectory = training_loop(model, optimizer, loss_fn, x_meas_tensor, y_obs_tensor, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over iterations\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(trajectory[:,-1], label=\"L_mse\")\n",
    "ax.set_xlim(left=0, right=len(trajectory)-1)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss (MSE)\")\n",
    "ax.set_title(\"Loss Curve\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Plot the path on the loss landscape\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(A, B, losses, levels=100, cmap='viridis')\n",
    "ax.set_xlim(A.min(), A.max())\n",
    "ax.set_ylim(B.min(), B.max())\n",
    "ax.set_xlabel(\"tau\")\n",
    "ax.set_ylabel(\"phi\")\n",
    "ax.set_title(\"Gradient Descent Path on Loss Landscape\")\n",
    "\n",
    "# Path\n",
    "ax.plot(trajectory[0, 0], trajectory[0, 1], 'rx', label=\"Initial Point\")\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', alpha=0.25, label=\"Gradient Descent Path\")\n",
    "ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', label=\"Final Point\")\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.scatter(x_meas_tensor.detach(), model(x_meas_tensor).detach(), s=6, label=\"predictions\")\n",
    "ax.scatter(x_meas_tensor.detach(), y_obs_tensor, s=6, label=\"observations\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c9d1d",
   "metadata": {},
   "source": [
    "So with this engineering issue sorted we can return to the science.\n",
    "\n",
    "So what happens if we have a more complicated function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46863d48",
   "metadata": {},
   "source": [
    "f_target = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3849094",
   "metadata": {},
   "source": [
    "Let's define the `Dataset` to wrap up `f_target_` and the observation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization loop aka. training loop\n",
    "def training_loop(model, optimizer, loss_fn, x, y, steps, thresh = None):\n",
    "    losses = []\n",
    "    for idx in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y, model(x))\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, torch.tensor(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9be770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, a: float, b: float, N = 100):\n",
    "        self.__f = lambda x: 2.04 * x**3 + 1.54 * x**2 - 0.54 * x + 1.3 # f : data generating process\n",
    "        self.x = torch.from_numpy(np.random.uniform(low=a, high=b, size=N)).to(dtype=torch.float32) # [N,1]\n",
    "        self.y = torch.from_numpy(observe_noisy(self.__f, self.x.numpy())).to(dtype=torch.float32)\n",
    "        self.x = self.x.unsqueeze(-1)\n",
    "        self.y = self.y.unsqueeze(-1)\n",
    "        self.x = (self.x - self.x.min()) / (self.x.max() - self.x.min())\n",
    "        self.y = (self.y - self.y.min()) / (self.y.max() - self.y.min())\n",
    "\n",
    "# define a dataset\n",
    "dataset = Dataset(a=-2, b=2, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the observations\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
    "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x_meas\")\n",
    "ax.set_ylabel(\"y_obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesisSpace(nn.Module):\n",
    "    def __init__(self, a: float, b: float):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.tensor([a]))\n",
    "        self.b = nn.Parameter(torch.tensor([b]))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.a * x + self.b\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.a.item():.2f} x^2 + {self.b.item():.2f} x + {self.c.item():.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ad5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.plot(losses, label=\"loss\")\n",
    "    ax.set_xlim(left=0, right=len(losses)-1)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss (MSE)\")\n",
    "    ax.set_title(\"Loss Curve\")\n",
    "    ax.grid(True, alpha=0.50)\n",
    "    ax.legend()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, dataset):\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    with torch.no_grad():\n",
    "        ax.scatter(dataset.x, model(dataset.x), s=4, label=\"predictions\")\n",
    "        ax.scatter(dataset.x, dataset.y, s=4, label=\"observations\")\n",
    "    ax.set_xlim(left=dataset.x.min(), right=dataset.x.max())\n",
    "    ax.grid(True, alpha=0.50)\n",
    "    ax.legend(loc=\"best\")\n",
    "    return fig, ax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial model\n",
    "model = HypothesisSpace(0.0, 0.0)\n",
    "\n",
    "# define optimizer : autograd handles computing gradients etc.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# perform the training loop\n",
    "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100, 0.1)\n",
    "\n",
    "# plot loss curve & predictions\n",
    "plot_loss_curve(losses)\n",
    "plot_predictions(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d059c6",
   "metadata": {},
   "source": [
    "So whats the issue here?\n",
    "\n",
    "Our optimization process has found the minimum solution for the hypothesis class but the error is still too high for it to be a suitable solution.\n",
    "\n",
    "Well clearly our hypothesis space is too restricted - it's unable to represent the function such that our final function is suitable - we can visualize this using spaces...\n",
    "\n",
    "We would say our approximation error is bounded - our poor choice of model class limits our ability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5a022",
   "metadata": {},
   "source": [
    "So what happens when our function has an unknown form? We instead need a way to define a hypothesis class that can represent lots of different functions - this is where universal function approximations come in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95854427",
   "metadata": {},
   "source": [
    "Lots of different sorts of function approximators etc. fourier curves, polynomial models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db011bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    f(x) = ReLU(Wx + B)\n",
    "    \"\"\"\n",
    "    def __init__(self, w: float, b: float):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        self.linear.weight = nn.Parameter(torch.tensor([[float(w)]])) # <-- vary the parameters manually\n",
    "        self.linear.bias = nn.Parameter(torch.tensor([[float(b)]]))\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.linear(x))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.linspace(-5, 5, 1000).unsqueeze(-1)\n",
    "    y = Perceptron(1.0, 0.0)(x)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.plot(x,y)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.set_title(\"ReLU Perceptron\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"f(x) = ReLU(wx + b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb5880",
   "metadata": {},
   "source": [
    "Multilayer perceptrons with ReLU basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7371a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    f(x) = ReLU(Wx + B)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "        # specific initialization\n",
    "        # self.linear.weight = nn.Parameter(torch.tensor([[1.0]])) # <-- define a specific starting model init\n",
    "        # self.linear.bias = nn.Parameter(torch.tensor([[0.0]]))\n",
    "\n",
    "        # random initialization\n",
    "        nn.init.uniform_(self.linear.weight, -1, 1)\n",
    "        nn.init.uniform_(self.linear.bias, -1, 1)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.linear(x))\n",
    "    \n",
    "\n",
    "model = Perceptron()\n",
    "dataset = Dataset(a=-2, b=2, N=100)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 1000)\n",
    "\n",
    "plot_loss_curve(losses)\n",
    "plot_predictions(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d04257",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer(nn.Module):\n",
    "    def __init__(self, N: int = 1):\n",
    "        super().__init__()\n",
    "        self.perceptrons = nn.ModuleList([Perceptron() for _ in range(N)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # y_pred = Perceptron0(x) + ... + PerceptronN(x)\n",
    "        y_pred = 0\n",
    "        for perceptron in self.perceptrons:\n",
    "            y_pred = y_pred + perceptron(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = PerceptronLayer(N=10)\n",
    "dataset = Dataset(a=-2, b=2, N=100)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 1000)\n",
    "\n",
    "plot_loss_curve(losses)\n",
    "plot_predictions(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914a212",
   "metadata": {},
   "source": [
    "Without different initialization the model likely collapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for perceptron in model.perceptrons:\n",
    "    print(perceptron.linear.weight.item(), perceptron.linear.bias.item())\n",
    "    # _ = plot_predictions(perceptron, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc409b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019ec2fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf596b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def plot_loss_landscape(model, dataset, param0_range, param1_range, res=100, param0_key=\"a\", param1_key=\"b\"):\n",
    "    # select parameters for landscape\n",
    "    param0 = torch.linspace(param0_range[0], param0_range[1], res)\n",
    "    param1 = torch.linspace(param1_range[0], param1_range[1], res)\n",
    "    P0, P1 = torch.meshgrid(param0, param1, indexing=\"ij\")\n",
    "\n",
    "    # state dict\n",
    "    _model = copy.deepcopy(model)\n",
    "    _state = model.state_dict()\n",
    "\n",
    "    # compute across model space\n",
    "    losses = np.zeros_like(P0)\n",
    "    for i in range(param0.shape[0]):\n",
    "        for j in range(param1.shape[0]):\n",
    "            # select a function from the hypothesis space by updating model state\n",
    "            _state[param0_key] = param0[i]\n",
    "            _state[param1_key] = param1[j]\n",
    "            _model.load_state_dict(_state)\n",
    "\n",
    "            # compute loss across dataset\n",
    "            losses[i,j] = loss_mse_fn(_model(dataset.x), dataset.y)\n",
    "    \n",
    "    # plot the loss landscape\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    _ = ax.contourf(P0, P1, losses, levels=100, cmap='viridis')\n",
    "    ax.set_xlim(P0.min(), P0.max())\n",
    "    ax.set_ylim(P1.min(), P1.max())\n",
    "    ax.set_xlabel(\"tau\")\n",
    "    ax.set_ylabel(\"phi\")\n",
    "    ax.set_title(\"Gradient Descent Path on Loss Landscape\")\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def plot_training_trajectory(ax, trajectory, param0_idx, param1_idx):\n",
    "    # Path\n",
    "    ax.plot(trajectory[0, param0_idx], trajectory[0, param1_idx], 'rx', label=\"Initial Point\")\n",
    "    ax.plot(trajectory[:, param0_idx], trajectory[:, param1_idx], 'r-', alpha=0.25, label=\"Gradient Descent Path\")\n",
    "    ax.plot(trajectory[-1, param0_idx], trajectory[-1, param1_idx], 'ro', label=\"Final Point\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
