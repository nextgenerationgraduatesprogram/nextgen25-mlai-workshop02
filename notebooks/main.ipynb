{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3781f022",
      "metadata": {
        "id": "3781f022"
      },
      "source": [
        "<h1>ü§ñ MLAI Workshop #02</h1>\n",
        "\n",
        "In the previous workshop, we explored a foundational drive behind machine learning ‚Äî thinking of it not just as code or models, but as a process of learning functions under constraints such as hypothesis, data and optimization. We emphasized the importance of the hypothesis space, how data shapes learning, and where errors can arise even when everything seems to be working correctly. Today, we‚Äôll start to address some limitations of our prior models and try to interpret the limitations in the context of the theory - and consequently, what we can do to address it.\n",
        "\n",
        "üí¨ *Question for the audience! Have you ever trained a model that just... wouldn‚Äôt learn ‚Äî and you weren‚Äôt sure why? What do you think the cause of failure was?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e112cacd",
      "metadata": {
        "id": "e112cacd"
      },
      "source": [
        "<h2>üóìÔ∏è Agenda</h2>\n",
        "\n",
        "1. **Recap & Refactor**  \n",
        "   Summarize key concepts from the previous workshop and refine our implementation using PyTorch's autograd and optimizers.\n",
        "\n",
        "2. **Flexible Function Approximators**  \n",
        "   Explore more expressive hypothesis spaces (e.g. neural networks) and understand their strengths and limitations in practice.\n",
        "\n",
        "3. **Modelling in Context**  \n",
        "   Apply these ideas to your own modelling task ‚Äî build, train, and interpret a model on our own functions.\n",
        "\n",
        "4. **Generalization & Limits**  \n",
        "   Investigate how models behave beyond training data. Where do they succeed? Where do they fail? How can we improve them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26165351",
      "metadata": {
        "id": "26165351"
      },
      "source": [
        "<h2>Summary</h2>\n",
        "\n",
        "1. Machine learning is the task of approximating an unknown function using data.\n",
        "2. Hypothesis Space: We define a family of functions (e.g. linear models, neural networks) that we‚Äôre willing to consider - this results in approximation error.\n",
        "3. Data as Constraints: Each data point reduces the set of plausible functions by adding constraints - this results in generalization error.\n",
        "4. Optimization: We use methods like gradient descent to minimize loss and fit the data - this results in optimization error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aee8175",
      "metadata": {
        "id": "9aee8175"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>üìà Section 1: What's wrong with our hypothesis space?</h2>"
      ],
      "metadata": {
        "id": "5cEDtpn9UNe_"
      },
      "id": "5cEDtpn9UNe_"
    },
    {
      "cell_type": "markdown",
      "id": "a5c49551",
      "metadata": {
        "id": "a5c49551"
      },
      "source": [
        "<h3>Section 1A. Revisiting Workshop #1</h3>\n",
        "\n",
        "So we suppose we have some target function we want to approximate.\n",
        "\n",
        "\\begin{align*}\n",
        "  f^{*}(x) = \\left\\{ \\mathbb{R} \\rightarrow \\mathbb{R} \\,|\\, f(x) = -0.86 x + 1.4 \\right\\} \\tag{1.1}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18b2fc6",
      "metadata": {
        "id": "f18b2fc6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77115786",
      "metadata": {
        "id": "77115786"
      },
      "outputs": [],
      "source": [
        "# target function\n",
        "def f_target(x):\n",
        "    return -0.68 * x + 1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd8caf9",
      "metadata": {
        "id": "1bd8caf9"
      },
      "source": [
        "We assume we cannot access this function, but we can observe its behaviour - and thus we decide to collect a dataset that hopefully captures this functions behaviour.\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{D} = \\left\\{ (\\mathcal{X}_{i}, \\mathcal{Y}_{i}) \\right\\}_{i=0}^{N} \\tag{1.2}\n",
        "\\end{align*}\n",
        "\n",
        "where:\n",
        "- $\\mathcal{X}_{i}$ represents an observation of the input.\n",
        "- $\\mathcal{Y}_{i}$ represents an observation of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82c28bb",
      "metadata": {
        "id": "a82c28bb"
      },
      "outputs": [],
      "source": [
        "# observe some data using a noisy observation process\n",
        "def observe_noisy(f, x):\n",
        "    # we often can't sample perfectly a point in the domain perfectly\n",
        "    x_noise = np.random.normal(0.05, 0.02, x.shape[0])\n",
        "    x_measure = x + x_noise\n",
        "\n",
        "    # we often can't measure the result perfectly\n",
        "    y_noise = np.random.normal(-0.05, 0.15, x.shape[0])\n",
        "    y_obs = f(x_measure) + y_noise\n",
        "\n",
        "    return y_obs\n",
        "\n",
        "# we select some points to observe the response at\n",
        "x_meas = np.random.uniform(low=-1, high=1, size=100)\n",
        "y_obs = observe_noisy(f_target, x_meas)\n",
        "\n",
        "# plot the observations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
        "ax.set_xlim(x_meas.min(), x_meas.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137855a7",
      "metadata": {
        "id": "137855a7"
      },
      "source": [
        "After some exploratory data analysis we might hypothesize this function has a linear relationship, and we design a very specific constricted hypothesis space to match this:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{H} = \\left\\{ f_{\\theta}: \\mathbb{R} \\to \\mathbb{R} \\,|\\, f_{\\theta}(x) = \\theta_{1} x + \\theta_{2} \\right\\} \\tag{1.3}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b4f49e",
      "metadata": {
        "id": "46b4f49e"
      },
      "outputs": [],
      "source": [
        "# construct a hypothesis space\n",
        "def H(a, b):\n",
        "    def f(x):\n",
        "        return a * x + b\n",
        "    return f"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d4d593e",
      "metadata": {
        "id": "6d4d593e"
      },
      "source": [
        "To provide an empirical evaluation of the fit of our function we define a loss function that meaningfully evaluates the task we want our model to perform:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}(\\mathcal{D}, f_\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(\\hat{y}_{i}, y_i\\right) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(f_\\theta(x_i), y_i\\right) \\tag{1.4}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437e5275",
      "metadata": {
        "id": "437e5275"
      },
      "outputs": [],
      "source": [
        "# define a loss function\n",
        "def dataset_avg_mse_loss(f, x, y):\n",
        "    # compute average mse loss between prediction `f(x)` and observation `y`\n",
        "    L = np.sum((f(x) - y) ** 2) / x.shape[0]\n",
        "    return L\n",
        "\n",
        "# evaluate a specific model\n",
        "L_mse = dataset_avg_mse_loss(H(1,1), x_meas, y_obs)\n",
        "L_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096900db",
      "metadata": {
        "id": "096900db"
      },
      "source": [
        "We then also define the target spaces as a constraint on our set of approximations we consider useful:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{T} = \\left\\{ f \\in \\mathcal{H} \\,|\\, \\mathcal{L}(\\mathcal{D}, f_{\\theta} \\leq \\epsilon) \\right\\} \\tag{1.5}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e5f8fc",
      "metadata": {
        "id": "77e5f8fc"
      },
      "outputs": [],
      "source": [
        "# function to check if the solution is suitable\n",
        "def is_suitable(L_mse, threshold):\n",
        "    return L_mse < threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f391de1",
      "metadata": {
        "id": "2f391de1"
      },
      "source": [
        "Since we have a simple model we can visualize the loss landscape in a meaningful manner before we begin the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9e0731",
      "metadata": {
        "id": "fa9e0731"
      },
      "outputs": [],
      "source": [
        "# visualize the loss landscape for understanding\n",
        "a = np.linspace(-5, 5, num=100) # <-- explore a different range, does [-5,5] let you find the optimal solution?\n",
        "b = np.linspace(-5, 5, num=100)\n",
        "A, B = np.meshgrid(a, b, indexing=\"ij\")\n",
        "\n",
        "# lets compute the loss (error across the dataset) for each function defined by the parameter\n",
        "losses = np.zeros_like(A)\n",
        "for i in range(a.shape[0]):\n",
        "    for j in range(b.shape[0]):\n",
        "        # select a function from the hypothesis space\n",
        "        f_approx = H(a=a[i], b=b[j])\n",
        "\n",
        "        # compute loss across dataset\n",
        "        losses[i,j] = dataset_avg_mse_loss(f_approx, x_meas, y_obs)\n",
        "\n",
        "# lets plot the result\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "s = ax.plot_surface(A, B, losses, cmap='viridis')\n",
        "ax.set_xlim(a.min(), a.max())\n",
        "ax.set_ylim(b.min(), b.max())\n",
        "ax.set_xlabel(\"a\")\n",
        "ax.set_ylabel(\"b\")\n",
        "ax.set_zlabel(\"Dataset Loss (MSE)\")\n",
        "ax.set_title(f\"Loss Landscape\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define the gradients of our loss function with respect to each parameter of our model, which we then use in the gradient descent algorithm.\n",
        "\n",
        "\\begin{align*}\n",
        "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(f_\\theta(x), y) \\tag{1.6}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "5BOxV34Wz7DX"
      },
      "id": "5BOxV34Wz7DX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c0e4d9",
      "metadata": {
        "id": "b4c0e4d9"
      },
      "outputs": [],
      "source": [
        "# derivate of function with respect to parameters\n",
        "def df_da(x, a, b):\n",
        "    return x\n",
        "\n",
        "def df_db(x, a, b):\n",
        "    return 1\n",
        "\n",
        "# derivate of loss function with respect to params : chain rule\n",
        "def dL_dtheta(x, y, a, b):\n",
        "    dL_da = 2 * np.mean((H(a, b)(x) - y) * df_da(x, a, b))\n",
        "    dL_db = 2 * np.mean((H(a, b)(x) - y) * df_db(x, a, b))\n",
        "    return (dL_da, dL_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start our optimization process at some initial point on the loss landscape $(\\theta_{0}, \\theta_{1})$, using the gradient information we then update the parameter values by $\\nabla_\\theta \\mathcal{L}(f_\\theta(x), y)$ where $\\nabla_\\theta$ represents a scaling of the gradients. We repeat this process for $N$ steps."
      ],
      "metadata": {
        "id": "pksAaoKR0KAC"
      },
      "id": "pksAaoKR0KAC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb47a58",
      "metadata": {
        "id": "6fb47a58"
      },
      "outputs": [],
      "source": [
        "# optimization using gradient descent\n",
        "def gradient_descent(a_init, b_init, x, y, lr, steps):\n",
        "    a, b = a_init, b_init\n",
        "    history = []\n",
        "\n",
        "    for idx in range(steps):\n",
        "        # compute gradients\n",
        "        dL_da, dL_db = dL_dtheta(x, y, a, b)\n",
        "\n",
        "        # curr loss\n",
        "        L_mse = dataset_avg_mse_loss(H(a, b), x, y)\n",
        "        history.append((a, b, L_mse))\n",
        "\n",
        "        # gradient descent update step\n",
        "        a = a - lr * dL_da\n",
        "        b = b - lr * dL_db\n",
        "\n",
        "    return np.array(history)\n",
        "\n",
        "# run gradient descent and get final parameters\n",
        "trajectory = gradient_descent(a_init=.0, b_init=.0, x=x_meas, y=y_obs, lr=0.1, steps=100, )\n",
        "f_approx = H(trajectory[-1,0], trajectory[-1,1])\n",
        "\n",
        "# exit condition : found suitable solution\n",
        "if is_suitable(trajectory[-1,-1], 0.01): print(f\"found suitable solution with loss L={trajectory[-1,-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33f655a",
      "metadata": {
        "id": "d33f655a"
      },
      "outputs": [],
      "source": [
        "# Plot loss over iterations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.plot(trajectory[:, 2], label=\"L_mse\")\n",
        "ax.set_xlim(left=0, right=len(trajectory)-1)\n",
        "ax.set_xlabel(\"Iteration\")\n",
        "ax.set_ylabel(\"Loss (MSE)\")\n",
        "ax.set_title(\"Loss Curve\")\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Plot the path on the loss landscape\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "contour = ax.contourf(A, B, losses, levels=100, cmap='viridis')\n",
        "ax.set_xlim(A.min(), A.max())\n",
        "ax.set_ylim(B.min(), B.max())\n",
        "ax.set_xlabel(\"a\")\n",
        "ax.set_ylabel(\"b\")\n",
        "ax.set_title(\"Gradient Descent Path on Loss Landscape\")\n",
        "\n",
        "# Path\n",
        "ax.plot(trajectory[0, 0], trajectory[0, 1], 'rx', label=\"Initial Point\")\n",
        "ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', alpha=0.25, label=\"Gradient Descent Path\")\n",
        "ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', label=\"Final Point\")\n",
        "ax.legend()\n",
        "\n",
        "# Plot predictions\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "ax.scatter(x_meas, f_approx(x_meas), s=6, label=\"predictions\")\n",
        "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
        "ax.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 1B. Refining Our Code</h3>\n",
        "\n",
        "Building everything from scratch helps us understand the mechanics of learning, but it quickly becomes tedious and error-prone as our models grow in complexity. Thankfully, libraries such as `torch` come with all of the functionality we need out of the box e.g the `autograd` engine to handle gradient computation.\n",
        "\n",
        "Let's convert this code over into a `torch` style implementation."
      ],
      "metadata": {
        "id": "rdHTN_iEUCuL"
      },
      "id": "rdHTN_iEUCuL"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Callable\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, f: Callable, a: float, b: float, N = 100):\n",
        "        self.__f = f\n",
        "        self.x = torch.from_numpy(np.random.uniform(low=a, high=b, size=N)).to(dtype=torch.float32)\n",
        "        self.y = torch.from_numpy(observe_noisy(self.__f, self.x.numpy())).to(dtype=torch.float32)\n",
        "        self.x = self.x.unsqueeze(-1) # [N,1]\n",
        "        self.y = self.y.unsqueeze(-1) # [N,1]\n",
        "\n",
        "        # <-- define some additional processing here if you want"
      ],
      "metadata": {
        "id": "4ntW_-yx4fs4"
      },
      "id": "4ntW_-yx4fs4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8a4109",
      "metadata": {
        "id": "7d8a4109"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HypothesisSpace(nn.Module):\n",
        "    \"\"\" implement a simple linear equation hypothesis space\n",
        "    f(x) = a * x + b\n",
        "    \"\"\"\n",
        "    def __init__(self, a: float, b: float):\n",
        "        super().__init__()\n",
        "        self.a = nn.Parameter(torch.tensor([a]))\n",
        "        self.b = nn.Parameter(torch.tensor([b]))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.a * x + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b021fd5",
      "metadata": {
        "id": "4b021fd5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# define the optimization loop aka. training loop\n",
        "def training_loop(model, optimizer, loss_fn, x, y, steps):\n",
        "    losses = []\n",
        "    with tqdm(range(steps)) as pbar:\n",
        "      for idx in pbar:\n",
        "          optimizer.zero_grad()\n",
        "          loss = loss_fn(y, model(x))\n",
        "          losses.append(loss)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          pbar.set_description(f\"loss: {loss.item():.3f}\")\n",
        "    return model, torch.tensor(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7029c3c0",
      "metadata": {
        "id": "7029c3c0"
      },
      "outputs": [],
      "source": [
        "# define a dataset\n",
        "dataset = Dataset(f=f_target, a=-2, b=2, N=100)\n",
        "\n",
        "# initial model\n",
        "model = HypothesisSpace(1.0, 1.0)\n",
        "\n",
        "# define optimizer : autograd handles computing gradients etc.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# define loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# perform the training loop\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "if is_suitable(losses[-1], 0.01): print(f\"found suitable solution with loss L={losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787c3a1b",
      "metadata": {
        "id": "787c3a1b"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curve(losses):\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "    ax.plot(losses, label=\"loss\")\n",
        "    ax.set_xlim(left=0, right=len(losses)-1)\n",
        "    ax.set_xlabel(\"Iteration\")\n",
        "    ax.set_ylabel(\"Loss (MSE)\")\n",
        "    ax.set_title(\"Loss Curve\")\n",
        "    ax.grid(True, alpha=0.50)\n",
        "    ax.legend()\n",
        "    return fig, ax\n",
        "\n",
        "def plot_predictions(model, dataset, ax = None):\n",
        "    if ax is None: fig, ax = plt.subplots(figsize=(8,3))\n",
        "    with torch.no_grad():\n",
        "        ax.scatter(dataset.x, model(dataset.x), s=4, label=\"predictions\")\n",
        "        ax.scatter(dataset.x, dataset.y, s=4, label=\"observations\")\n",
        "    ax.set_xlim(left=dataset.x.min(), right=dataset.x.max())\n",
        "    ax.grid(True, alpha=0.50)\n",
        "    ax.legend(loc=\"best\")\n",
        "    return ax\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 1C. What happens when our function gets more complicated?</h3>\n",
        "\n",
        "So with this engineering issue sorted, lets begin to explore what happens when our target function becomes more complicated. Consider we have some slighty more complicated target function:\n",
        "\n",
        "\\begin{align*}\n",
        "  f^{*}(x) = \\left\\{ \\mathbb{R} \\rightarrow \\mathbb{R} \\,|\\, f(x) = 1.54x^{2} - 0.54x + 1.3 \\right\\} \\tag{1.7}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "3wyCmoOEUe5Q"
      },
      "id": "3wyCmoOEUe5Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9be770",
      "metadata": {
        "id": "ef9be770"
      },
      "outputs": [],
      "source": [
        "# define a dataset\n",
        "f_quadratic = lambda x: 1.54 * x**2 - 0.54 * x + 1.3\n",
        "dataset = Dataset(f=f_quadratic, a=-2, b=2, N=100)\n",
        "\n",
        "# visualize the observations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
        "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f65c8205",
      "metadata": {
        "id": "f65c8205"
      },
      "outputs": [],
      "source": [
        "class HypothesisSpace(nn.Module):\n",
        "    def __init__(self, a: float, b: float): # <-- add some inputs\n",
        "        super().__init__()\n",
        "        self.a = nn.Parameter(torch.tensor([a])) # <-- add some extra parameters\n",
        "        self.b = nn.Parameter(torch.tensor([b]))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <-- rewrite the hypothesis space\n",
        "        return self.a * x + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2620a8a1",
      "metadata": {
        "id": "2620a8a1"
      },
      "outputs": [],
      "source": [
        "# initial model\n",
        "model = HypothesisSpace(0.0, 0.0)\n",
        "\n",
        "# define optimizer : autograd handles computing gradients etc.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# perform the training loop\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "# plot loss curve & predictions\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d059c6",
      "metadata": {
        "id": "c3d059c6"
      },
      "source": [
        "We assume in this scenario that both the optimization process and the dataset are not limiting factors ‚Äî the model is training properly, and the data is representative. Yet, we still fail to achieve satisfactory performance. In this case, the limitation lies in the **approximation error**: the best possible function within our hypothesis space is still far from the true underlying function. This occurs because our hypothesis space is too restrictive. For instance, a linear model cannot effectively approximate a cubic polynomial ‚Äî the complexity of the true function exceeds the representational capacity of the model.\n",
        "\n",
        "This is a key insight: even perfect optimization can't help if we're searching within the wrong space. To improve performance, we must **redefine our hypothesis space** ‚Äî expand it to include more expressive functions that can better capture the structure in the data.\n",
        "\n",
        "üëâ **Try this yourself:** Modify your model to use a more flexible hypothesis space (e.g. a higher-degree polynomial) and observe how performance changes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "c7pTdzZlXlKJ"
      },
      "id": "c7pTdzZlXlKJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Section 2. Universal Function Approximators</h2>\n",
        "\n",
        "Clearly, the process of repeatedly modifying our hypothesis space isn‚Äôt a scalable strategy for learning highly complex functions. In many real-world tasks, the true relationship between input $\\mathcal{X}$ and output $\\mathcal{Y}$ is unknown, nonlinear, and potentially very intricate. We rarely have a closed-form expression for this mapping ‚Äî and even if it exists, we typically cannot guess its structure reliably.\n",
        "\n",
        "To address this challenge, we turn to more **flexible function classes** that are capable of approximating a wide variety of functions. Conceptually, this means expanding our hypothesis space $\\mathcal{H}$ so that it can make more effective use of the parameter space $\\Theta$, and better fit the patterns within the data."
      ],
      "metadata": {
        "id": "C5IV-DBp7DsH"
      },
      "id": "C5IV-DBp7DsH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 2A. Universal Function Approximation</h3>\n",
        "\n",
        "What we seek is a **universal function approximator**: a model that, under certain conditions, can approximate *any* continuous function on a bounded domain to *arbitrary accuracy*, given sufficient capacity ‚Äî for example, enough neurons, basis functions, or ensemble components.\n",
        "\n",
        "Examples of such expressive hypothesis spaces include:\n",
        "- **Multilayer perceptrons (neural networks)**, which construct complex nonlinear mappings by composing layers of simple transformations (e.g., linear projections + nonlinear activations).\n",
        "- **Kernel methods** with radial basis functions (RBF), which implicitly map data into infinite-dimensional feature spaces to model complex, nonlinear relationships.\n",
        "\n",
        "These flexible spaces are what empower modern machine learning systems to perform across a wide range of tasks ‚Äî from image recognition to natural language processing ‚Äî without the need to hand-design the function form in advance.\n",
        "\n",
        "> Universal function approximators shift the burden of function design from the human to the learning algorithm.\n",
        "\n",
        "Let‚Äôs now look at one such model in detail and explore how it achieves this.\n",
        "\n"
      ],
      "metadata": {
        "id": "LxAMZYgOVHEq"
      },
      "id": "LxAMZYgOVHEq"
    },
    {
      "cell_type": "markdown",
      "id": "95854427",
      "metadata": {
        "id": "95854427"
      },
      "source": [
        "<h3>Section 2B. Linear Transformation with Recitified Linear Units (ReLU)</h3>\n",
        "\n",
        "\\begin{align*}\n",
        "  f(x) = ReLU(w_i x + b_i) \\tag{2.1}\n",
        "\\end{align*}\n",
        "\n",
        "We've seen what linear transformation does - what about this ReLU component? Each ReLU unit introduces a kink ‚Äî a non-smooth point ‚Äî at the location where its input changes sign. For a neuron computing $\\text{ReLU}(w_i x + b_i)$, this kink occurs at the hyperplane $w_i x + b_i = 0$.\n",
        "\n",
        "\\begin{align*}\n",
        "  \\text{ReLU}(x) = \\max(0, x) \\tag{2.2}\n",
        "\\end{align*}\n",
        "\n",
        "This divides the input space into **two linear regions**:\n",
        "- One where the neuron is active ($w_i x + b_i > 0$)\n",
        "- One where it is inactive ($w_i x + b_i \\leq 0$)\n",
        "\n",
        "Lets visualize this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db011bb3",
      "metadata": {
        "id": "db011bb3"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    f(x) = ReLU(Wx + B)\n",
        "    \"\"\"\n",
        "    def __init__(self, w: float, b: float):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "        self.linear.weight = nn.Parameter(torch.tensor([[float(w)]]))\n",
        "        self.linear.bias = nn.Parameter(torch.tensor([[float(b)]]))\n",
        "        self.act = nn.ReLU() # <-- try different activation functions e.g. Tanh, SiLU, GeLU\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.linear(x))\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = torch.linspace(-5, 5, 1000).unsqueeze(-1)\n",
        "    y = Perceptron(1.0, 0.0)(x) # <-- vary the parameters of the perceptron\n",
        "\n",
        "    # plot the perceptron function\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "    ax.plot(x,y)\n",
        "    ax.set_xlim(x.min(), x.max())\n",
        "    ax.set_ylim(-2, 5)\n",
        "    ax.set_title(\"ReLU Perceptron\")\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"f(x) = ReLU(wx + b)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60bb5880",
      "metadata": {
        "id": "60bb5880"
      },
      "source": [
        "We can explore optimizing a single perceptron for our scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7371a4",
      "metadata": {
        "id": "1a7371a4"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    f(x) = ReLU(Wx + B)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "        # specific initialization\n",
        "        self.linear.weight = nn.Parameter(torch.tensor([[1.0]])) # <-- define a specific starting model init\n",
        "        self.linear.bias = nn.Parameter(torch.tensor([[0.0]]))\n",
        "\n",
        "        # random initialization\n",
        "        # nn.init.uniform_(self.linear.weight, -1, 1)\n",
        "        # nn.init.uniform_(self.linear.bias, -1, 1)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.linear(x))\n",
        "\n",
        "\n",
        "# define the model, dataset, optimizer, and train the model\n",
        "model = Perceptron()\n",
        "dataset = Dataset(f=f_quadratic, a=-2, b=2, N=100)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "# visualize the losses and predictions of the trained model\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d04257",
      "metadata": {
        "id": "71d04257"
      },
      "source": [
        "<h3>Section 2C. Single Layer Perceptron Model</h3>\n",
        "\n",
        "Obviously, a single perceptron is quite limited in what it can approximate ‚Äî it can only model simple linear relationships within a region defined by its activation boundary, or **kink**, in the input space.\n",
        "\n",
        "However, when we introduce **multiple perceptron units**, each with its own activation boundary, these kinks collectively partition the input domain into distinct regions. Within each region, the network behaves linearly ‚Äî but across regions, it can express **nonlinear behavior** by stitching together these local linear segments.\n",
        "\n",
        "This gives rise to a **piecewise linear function**: a continuous function composed of linear components. In the **theoretical limit**, a **single-hidden-layer ReLU network with infinite width** becomes a **universal function approximator**. Under suitable conditions (such as proper weight initialization and scaling), it can approximate any continuous function on a compact input domain to arbitrary accuracy.\n",
        "\n",
        "We‚Äôll now explore how this works in practice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8d3647",
      "metadata": {
        "id": "0d8d3647"
      },
      "outputs": [],
      "source": [
        "class PerceptronLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    f(x) = Perceptron0(x) + ... + PerceptronN(x)\n",
        "    \"\"\"\n",
        "    def __init__(self, N: int = 1):\n",
        "        super().__init__()\n",
        "        self.perceptrons = nn.ModuleList([Perceptron() for _ in range(N)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y_pred = 0\n",
        "        for perceptron in self.perceptrons:\n",
        "            y_pred = y_pred + perceptron(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model = PerceptronLayer(N=2) # <-- lets define 2 perceptrons initially\n",
        "dataset = Dataset(f=f_quadratic, a=-2, b=2, N=100)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this align with what you expected? What do you think is the issue?"
      ],
      "metadata": {
        "id": "XYxlMFWB_DSs"
      },
      "id": "XYxlMFWB_DSs"
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    x = torch.linspace(-5, 5, 1000).unsqueeze(-1)\n",
        "\n",
        "    # plot the perceptron function\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "\n",
        "    for idx, perceptron in enumerate(model.perceptrons):\n",
        "        ax.plot(x, perceptron(x), label=f\"perceptron {idx}\", alpha=0.75)\n",
        "\n",
        "    ax.set_xlim(x.min(), x.max())\n",
        "    ax.set_ylim(-2, 5)\n",
        "    ax.set_title(\"ReLU Perceptron\")\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"f(x) = ReLU(wx + b)\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(True, alpha=0.50)"
      ],
      "metadata": {
        "id": "AMw8-03i-5Dq"
      },
      "id": "AMw8-03i-5Dq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization plays a critical role in neural network training. If all weights are initialized to the same value, the network becomes symmetric ‚Äî meaning all neurons in a given layer perform exactly the same computation and receive identical gradients during backpropagation. As a result the model behaves like it has just one effective unit.\n",
        "\n",
        "üëâ **Try this yourself:** 1. Modify the way the network weights are initialized to a random uniform distribution to give the network some variety. How does the model perform now?\n",
        "\n",
        "üëâ **Try this yourself:** 2. Are two perceptrons enough to represent this target function? Will increasing the number of Perceptrons improve the performance?\n",
        "\n",
        "üëâ **Try this yourself:** 3. There might still be a large number of neurons which aren't activating? How does the data distribution impact whether a given Perceptron is activated or not? How is the dataset scaled in relation to the input distribution? Can we use a different activation function to provide some gradient information?"
      ],
      "metadata": {
        "id": "XZm2kSD5YY10"
      },
      "id": "XZm2kSD5YY10"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>2D. Learning to fit MORE complex functions</h3>\n",
        "\n",
        "So clearly there are several important considerations when designing and training a ReLU network. While the **ReLU activation** gives us the ability to build piecewise linear approximations, the effectiveness of learning still depends heavily on:\n",
        "\n",
        "- The **basis functions** we choose (i.e., the architecture and activation function),\n",
        "- The **initialization** of the parameters, and\n",
        "- The **scaling and structure** of the dataset.\n",
        "\n",
        "Even though ReLU networks are universal function approximators in theory, in practice they are only as effective as our ability to **optimize their parameters**. Poor initialization (e.g., setting all weights to the same value) can lead to **symmetry** in the network ‚Äî where every neuron behaves identically and no useful function approximation is learned. Similarly, improper scaling of inputs can push most activations into a \"dead zone\", especially with ReLU, where gradients vanish and learning stalls.\n",
        "\n",
        "As we move toward fitting more complex functions ‚Äî ones with curvature, nonlinearity, or high-frequency variation ‚Äî we need to ensure that the network is **sufficiently expressive**, but also **well-initialized** and **well-conditioned** to allow optimization to succeed.\n",
        "\n",
        "In the next activity, we‚Äôll explore how to modify our networks to better approximate more complex target functions and investigate what architectural and training decisions make the biggest impact."
      ],
      "metadata": {
        "id": "zWQtlAuyCRmO"
      },
      "id": "zWQtlAuyCRmO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider another slighty more complicated target function:\n",
        "\n",
        "\\begin{align*}\n",
        "  f^{*}(x) = \\left\\{ \\mathbb{R} \\rightarrow \\mathbb{R} \\,|\\, f(x) = 2.04x^{3} + 1.54x^{2} - 0.54x + 1.3 \\right\\} \\tag{2.3}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "MhOAwbqDCo_K"
      },
      "id": "MhOAwbqDCo_K"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a dataset\n",
        "f_cubic = lambda x: 2.04 * x**3 + 1.54 * x**2 - 0.54 * x + 1.3\n",
        "dataset = Dataset(f=f_cubic, a=-2, b=2, N=100)\n",
        "\n",
        "# visualize the observations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
        "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ],
      "metadata": {
        "id": "2yZ7e6A4CQ90"
      },
      "id": "2yZ7e6A4CQ90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    f(x) = ReLU(Wx + B)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1) # <-- torch uses `kaiming` init for us\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.linear(x))\n",
        "\n",
        "\n",
        "class PerceptronLayer(nn.Module):\n",
        "    def __init__(self, N: int = 1):\n",
        "        super().__init__()\n",
        "        self.perceptrons = nn.ModuleList([Perceptron() for _ in range(N)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y_pred = 0\n",
        "        for perceptron in self.perceptrons:\n",
        "            y_pred = y_pred + perceptron(x)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "GAbfKDmADHia"
      },
      "id": "GAbfKDmADHia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PerceptronLayer(N=2) # <-- lets define 2 perceptrons initially\n",
        "dataset = Dataset(f=f_cubic, a=-2, b=2, N=100)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "Q4r4tiQfDc3Q"
      },
      "id": "Q4r4tiQfDc3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 2E. Linear Combinations of Perceptrons</h3>\n",
        "\n",
        "ReLU functions are **piecewise linear and convex**. As a result, a single-layer ReLU network (a shallow network) constructs its output by summing **convex, piecewise linear components**. This makes it inherently biased toward representing functions that are **convex or composed of sharp transitions** ‚Äî but not naturally suited to expressing smooth **concave** curves like quadratics or sines.\n",
        "\n",
        "To model concavity using only ReLU units, the network must carefully combine many \"bent\" ReLU functions with precisely tuned weights and biases ‚Äî a process that requires **many units**, and is **difficult to optimize effectively**.\n",
        "\n",
        "To provide more flexibility to our network, we can introduce a **linear output (readout) layer**. This allows the model to form **both convex and concave** outputs by combining the activations of ReLU units using **both positive and negative weights**. Instead of relying on each ReLU to fit a piece of the function directly, the network can now learn a richer combination of features ‚Äî dramatically increasing its expressivity, even in shallow architectures."
      ],
      "metadata": {
        "id": "7_pNogkyEo0F"
      },
      "id": "7_pNogkyEo0F"
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, act: bool = True):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.act = nn.ReLU() if act else lambda x: x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PerceptronLayer(nn.Module):\n",
        "    def __init__(self, N: int = 1):\n",
        "        super(PerceptronLayer, self).__init__()\n",
        "        modules = [Perceptron(1, N)]\n",
        "        modules.append(Perceptron(N, 1, act=False))\n",
        "        self.layers = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "model = PerceptronLayer(N=2) # <-- lets define 2 perceptrons initially\n",
        "dataset = Dataset(f=f_cubic, a=-2, b=2, N=100)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100) # <-- does the loss look like it could go lower, should we train for longer?\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "X4yOU3u0c1Tt"
      },
      "id": "X4yOU3u0c1Tt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This architectural tweak ‚Äî adding a linear combination on top of non-linear activations ‚Äî is a simple way to overcome the limitations imposed by ReLU's inherent convexity."
      ],
      "metadata": {
        "id": "RblOOFoadPM-"
      },
      "id": "RblOOFoadPM-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 2E. Multi Layer Perceptron Model</h3>\n",
        "\n",
        "Clearly, making this adjustment provides significantly more expressive power to our network, allowing it to more easily represent complex shapes ‚Äî including those with both convex and concave regions.\n",
        "\n",
        "We can further enhance this expressivity by introducing **multiple layers**. Each additional layer allows the network to learn a **piecewise linear approximation of the previous layer‚Äôs piecewise linear approximation**. In effect, this means the network can begin to capture **nonlinear interactions and curvature** through the **composition of simple functions**."
      ],
      "metadata": {
        "id": "U1AzmzaIbtRQ"
      },
      "id": "U1AzmzaIbtRQ"
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, act: bool = True):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.act = nn.ReLU() if act else lambda x: x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self, hidden_layers: int = 0, hidden_dim: int = 1):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "\n",
        "        # Create input layer\n",
        "        modules = [Perceptron(1, hidden_dim)]\n",
        "\n",
        "        # Create hidden layers\n",
        "        for _ in range(hidden_layers):\n",
        "            modules.append(Perceptron(hidden_dim, hidden_dim))\n",
        "\n",
        "        # Create output layer\n",
        "        modules.append(Perceptron(hidden_dim, 1, act=False)) # just perform linear aggregation and no non-linearity\n",
        "\n",
        "        # Create the model\n",
        "        self.layers = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9QKlqGn3FA_F"
      },
      "id": "9QKlqGn3FA_F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLayerPerceptron(hidden_layers=0, hidden_dim=2) # <-- lets define 2 perceptrons initially\n",
        "dataset = Dataset(f=f_cubic, a=-2, b=2, N=100)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # <-- how do different optimizers impact performance\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "gPKL6R8HDdpQ"
      },
      "id": "gPKL6R8HDdpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This compositional structure enables deeper networks to fit functions like **cubics, smooth oscillations, or piecewise-polynomial curves** ‚Äî tasks that would otherwise require an impractically large number of units in a shallow network. Instead of needing to directly ‚Äúdraw‚Äù every bump or bend with individual neurons, the network learns reusable transformations that **build up complexity progressively**.\n",
        "\n",
        "> In short: a linear output layer gives us flexibility in combining features, and depth gives us the capacity to build increasingly rich representations."
      ],
      "metadata": {
        "id": "QkSdXAiTbYZ-"
      },
      "id": "QkSdXAiTbYZ-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>An aside</h3>"
      ],
      "metadata": {
        "id": "bzkyo68igY4Q"
      },
      "id": "bzkyo68igY4Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also interesting to note that as we make our model larger, it becomes more capable of fitting complex data distributions. Bigger models have more representational capacity, which allows them to capture subtle variations in the dataset that smaller models might miss.\n",
        "\n",
        "This relates closely to the **Lottery Ticket Hypothesis**, which suggests that within large networks, there often exist smaller, well-initialized sub-networks ‚Äî \"winning tickets\" ‚Äî that are capable of learning the task effectively. As we increase the size of the model, the **probability of finding such a sub-network increases**, even if only a portion of the network is initially well-suited to the task.\n",
        "\n",
        "Counterintuitively, **larger models can also make optimization easier**. In low-dimensional parameter spaces, local minima can trap optimization. But in higher-dimensional spaces, those same points often become **saddle points**, allowing the optimizer to escape and continue improving. The increased redundancy and flexibility of large models can smooth out the loss landscape, providing **multiple paths to good solutions**.\n",
        "\n",
        "However, this advantage only materializes if the **optimization procedure is robust enough** to effectively explore the high-dimensional parameter space.\n",
        "\n",
        "> In short: bigger models aren't just more expressive ‚Äî they can also be *easier to train*, provided the optimizer is equipped to navigate the landscape."
      ],
      "metadata": {
        "id": "zXA_-S2kfqAt"
      },
      "id": "zXA_-S2kfqAt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Section 3. Putting It Into Practice!</h2>\n",
        "\n",
        "We're going to break into groups and explore both sides of the learning process: **the data** and **the model**. Your task is to create a challenging function - one that‚Äôs complex, nonlinear, and difficult to approximate - potentially even discontinuous - and then build a model that can learn to fit it. Then we're going to try and see just how good this approximation is."
      ],
      "metadata": {
        "id": "k64fD0z_FpSV"
      },
      "id": "k64fD0z_FpSV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 3A. Fit Your Own Function</h3>\n",
        "\n",
        "1. **Design a Complex Function**  \n",
        "   Work together to create a synthetic function with as much structure, curvature, or discontinuity as you'd like. The goal is to make the function *as contorted and expressive as possible* ‚Äî think beyond simple polynomials!\n",
        "\n",
        "2. **Generate Sample Data**  \n",
        "   Sample a set of input-output pairs from your function. You can optionally add noise to simulate real-world data.\n",
        "\n",
        "3. **Build a Model to Fit It**  \n",
        "   Use your understanding of model capacity, activation functions, and architecture to design a neural network that can approximate your function.  \n",
        "\n",
        "4. **Train & Evaluate**  \n",
        "   Try fitting your model to your data using what you've learned so far. Plot the results and report back as a group on your finding."
      ],
      "metadata": {
        "id": "a9MQsdPQen-j"
      },
      "id": "a9MQsdPQen-j"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a dataset\n",
        "f_yours = lambda x: ... # <-- go wild!\n",
        "dataset = Dataset(f=f_yours, ...)\n",
        "\n",
        "# visualize the observations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
        "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ],
      "metadata": {
        "id": "YE8EjMJzFovn"
      },
      "id": "YE8EjMJzFovn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ... # <-- define your model here\n",
        "dataset = ... # <-- define your dataset here\n",
        "optimizer = ... # <-- define your optimizer here\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, ...)\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "K9Zt9EozEndj"
      },
      "id": "K9Zt9EozEndj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reconvene and share with the class."
      ],
      "metadata": {
        "id": "9owL9F0_GdVv"
      },
      "id": "9owL9F0_GdVv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 3B. How well can your model interpolate?</h3>\n",
        "\n",
        "Neural networks, particularly with ReLU or smooth activations, tend to learn **smooth functions** that fit the data. Given enough data points that sufficiently cover the input space (i.e., fine resolution across the data manifold), they can interpolate well between samples ‚Äî essentially ‚Äúconnecting the dots‚Äù in a reasonable way. This smoothness bias helps the model generalize **within** the distribution ‚Äî but it doesn‚Äôt help beyond it."
      ],
      "metadata": {
        "id": "jzZJXVAWiup7"
      },
      "id": "jzZJXVAWiup7"
    },
    {
      "cell_type": "code",
      "source": [
        "# define your domain\n",
        "dataset = Dataset(f=f_yours, ...)\n",
        "\n",
        "# lets erase some of the data\n",
        "x_min, x_max = -0.1, 0.1\n",
        "keep_idxs = (dataset.x < x_min) | (dataset.x > x_max)\n",
        "dataset.x = dataset.x[keep_idxs].unsqueeze(-1)\n",
        "dataset.y = dataset.y[keep_idxs].unsqueeze(-1)\n",
        "\n",
        "# visualize the observations\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
        "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ],
      "metadata": {
        "id": "IdrxLvNYiytg"
      },
      "id": "IdrxLvNYiytg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLayerPerceptron(hidden_layers=1, hidden_dim=20) # <-- use the same parameters as before\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 1000)\n",
        "\n",
        "plot_loss_curve(losses)"
      ],
      "metadata": {
        "id": "30hbwJ1Ciyv1"
      },
      "id": "30hbwJ1Ciyv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intra-disitribution loss\n",
        "dataset = Dataset(f=f_yours, a=-2, b=2, N=100)\n",
        "mask = (dataset.x < x_min) | (dataset.x > x_max)\n",
        "dataset.x = dataset.x[mask].unsqueeze(-1)\n",
        "dataset.y = dataset.y[mask].unsqueeze(-1)\n",
        "ax = plot_predictions(model, dataset)\n",
        "print(f\"in-distribution loss: {loss_fn(model(dataset.x), dataset.y):.3f}\")\n",
        "\n",
        "# out-of-distribution loss\n",
        "dataset = Dataset(f=f_yours, a=-2, b=2, N=100)\n",
        "mask = (dataset.x > x_min) & (dataset.x < x_max)\n",
        "dataset.x = dataset.x[mask].unsqueeze(-1)\n",
        "dataset.y = dataset.y[mask].unsqueeze(-1)\n",
        "ax = plot_predictions(model, dataset, ax)\n",
        "print(f\"out-of-distribution loss: {loss_fn(model(dataset.x), dataset.y):.3f}\")\n",
        "ax.set_xlim(-2, 2)"
      ],
      "metadata": {
        "id": "zyO2K5cPiyym"
      },
      "id": "zyO2K5cPiyym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding where your model performs poorly provides insight into where the dataset has poorly constrained the model and thus where to gather more data. This discussion on how sufficiently and accurately we resolve the data manifold is a crucial aspect of machine learning - in approximation theory e.g. polynomial interpolation, error bounds of the function approximation often depend on the maximum spacing between samples:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\epsilon \\propto \\delta^{k} \\cdot || f^{(k)} ||_{\\infty}, \\quad\n",
        "    \\delta = max_{i}(\\mathcal{X}_{i+1} - \\mathcal{X}_{i}) \\tag{3.1}\n",
        "\\end{align*}\n",
        "\n",
        "where:\n",
        "- $\\epsilon$ is the error bound\n",
        "- $f^{(k)}$ is the $k$-th derivative of a function $f$\n",
        "- $\\delta$ is the largest distance between points\n",
        "\n",
        "Where a functions value changes rapidly (high $f^{(k)}$) to make sure we can constrain our function (minimize $\\epsilon$) we need to sample the points more densely around this area of change (minimize $\\delta$). Interpolation is where neural networks shine ‚Äî but only when your data gives them something to work with."
      ],
      "metadata": {
        "id": "n_i_SatJjAbq"
      },
      "id": "n_i_SatJjAbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 3C. How well can your model extrapolate?</h3>"
      ],
      "metadata": {
        "id": "5imEvU4keZ6x"
      },
      "id": "5imEvU4keZ6x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to evaluate not just how well it fits the **training data**, but how well it generalizes. So far, your model has only seen inputs within a certain range $\\mathcal{D}$ ‚Äî the domain of your sampled data. But real-world models often need to make predictions on inputs **outside this range**, to generalize to unseen cases or to be robust to adversarial examples.\n",
        "\n",
        "Let's see if this is the case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aBagSnhdGi1c"
      },
      "id": "aBagSnhdGi1c"
    },
    {
      "cell_type": "code",
      "source": [
        "# in-distribution\n",
        "id_dataset = Dataset(f=f_yours, ...) # <-- define your in-distribution data\n",
        "ax = plot_predictions(model, id_dataset)"
      ],
      "metadata": {
        "id": "KVaqtAZBGiKx"
      },
      "id": "KVaqtAZBGiKx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out-of-distribution data\n",
        "ood_dataset = Dataset(f=f_yours, ...) # <-- in-distribution data + out-of-distribution data i.e. bigger domain\n",
        "ax = plot_predictions(model, ood_dataset)"
      ],
      "metadata": {
        "id": "umRDfBX1fgEv"
      },
      "id": "umRDfBX1fgEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically the capacity of the networks are leveraged where the data is well defined to force the geometry of the networks activations to fit the geometry of the data manifold.\n",
        "\n",
        "Extrapolation is difficult because the model receives **no supervision or constraints** in regions outside the training distribution. Unless the network has explicitly learned the **underlying rules** or structure of the data (e.g., physical laws, symmetries, or inductive priors), it has no reason to behave sensibly in unfamiliar regions.\n",
        "\n",
        "In most practical settings, the model is just an **approximation**: it captures patterns in the data, but not the principles that generate them. As a result, its behavior outside the training domain is often **unconstrained and unpredictable**.\n",
        "\n",
        "Understanding where your model performs poorly ‚Äî especially on out-of-distribution (OOD) inputs ‚Äî gives valuable feedback about **which parts of the input space are under-constrained**. If you believe the model has enough capacity to approximate the target function, poor generalization usually means you need to:\n",
        "\n",
        "- **Collect more data** in those regions,\n",
        "- **Resample** with better coverage,\n",
        "- Or **introduce structure** into the model (e.g., domain knowledge, architectural bias).\n",
        "\n",
        "> Good generalization is not just about model size ‚Äî it‚Äôs about the match between your data distribution and the problem you‚Äôre trying to solve.\n"
      ],
      "metadata": {
        "id": "pRrH9GNrHFX5"
      },
      "id": "pRrH9GNrHFX5"
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLayerPerceptron(hidden_layers=1, hidden_dim=10) # <-- use the same parameters as before\n",
        "dataset = ood_dataset\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset.x, dataset.y, 100)\n",
        "\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "glGhLvzIh811"
      },
      "id": "glGhLvzIh811",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are generally good at **interpolating** within the range of their training data ‚Äî but they often struggle to **extrapolate** beyond it. This is a fundamental limitation, rooted in how these models learn."
      ],
      "metadata": {
        "id": "a9v7uM7ZhzM2"
      },
      "id": "a9v7uM7ZhzM2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Summary</h2>\n",
        "\n",
        "1. **Learning as Function Approximation**: Machine learning is about learning a function that maps inputs to outputs using data, assumptions, and optimization.\n",
        "2. **Hypothesis Spaces & Universal Approximators**: Neural networks are powerful because they can approximate a wide range of functions ‚Äî given enough capacity and the right architecture.\n",
        "3. **Practical Exploration**: A model's ability to learn depends not just on the architecture, but on how it's initialized, constrained, and trained ‚Äî and how well the data represents the task.\n",
        "4. **Interpolation vs Extrapolation**: Neural networks generalize well within the training range (interpolation), but fail unpredictably outside it (extrapolation).\n",
        "5. **Final Takeaway**: Building effective models means balancing expressivity, data coverage, and optimization ‚Äî and always testing their limits.\n",
        "\n",
        "In the next workshop we'll explore applying neural networks to higher dimensional inputs and solving real-world problems, with a focus on understanding the limitations of these systems and building up intuition about how to address them."
      ],
      "metadata": {
        "id": "IsQkeWfAkHvC"
      },
      "id": "IsQkeWfAkHvC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLFwsWnpkeuL"
      },
      "id": "iLFwsWnpkeuL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}